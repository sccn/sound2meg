{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eddaaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import csv\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# For visualize input\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import io\n",
    "import torchvision\n",
    "from torchvision import transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Net(nn.Module):   \n",
    "  def __init__(self, F_out, inchans, outchans, K):\n",
    "    super().__init__()\n",
    "    self.D2 = 320\n",
    "    self.outchans = outchans\n",
    "    #self.spatial_attention = SpatialAttention(inchans, outchans, K, montage[:,0], montage[:,1])\n",
    "    self.conv = nn.Conv2d(outchans, outchans, 1, padding='same')\n",
    "    self.conv_blocks = nn.Sequential(*[self.generate_conv_block(k) for k in range(5)]) # 5 conv blocks\n",
    "    self.final_convs = nn.Sequential(\n",
    "      nn.Conv2d(self.D2, self.D2*2, 1),\n",
    "      nn.GELU(),\n",
    "      nn.Conv2d(self.D2*2, F_out, 1)\n",
    "    )\n",
    "    self.l1 = nn.Linear(256*F_out, 2)\n",
    "    \n",
    "  def generate_conv_block(self, k):\n",
    "    kernel_size = (1,3)\n",
    "    padding = 'same' # (p,0)\n",
    "    return nn.Sequential(OrderedDict([\n",
    "      ('conv1', nn.Conv2d(self.outchans if k==0 else self.D2, self.D2, kernel_size, dilation=pow(2,(2*k)%5), padding=padding)),\n",
    "      ('bn1',   nn.BatchNorm2d(self.D2)), \n",
    "      ('gelu1', nn.GELU()),\n",
    "      ('conv2', nn.Conv2d(self.D2, self.D2, kernel_size, dilation=pow(2,(2*k+1)%5), padding=padding)),\n",
    "      ('bn2',   nn.BatchNorm2d(self.D2)),\n",
    "      ('gelu2', nn.GELU()),\n",
    "      ('conv3', nn.Conv2d(self.D2, self.D2*2, kernel_size, padding=padding)),\n",
    "      ('glu',   nn.GLU(dim=1))\n",
    "    ]))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x[:,0].unsqueeze(3) # add dummy dimension at the end\n",
    "    x = self.conv(x)\n",
    "        \n",
    "    for k in range(len(self.conv_blocks)):\n",
    "      if k == 0:\n",
    "        x = self.conv_blocks[k](x)\n",
    "      else:\n",
    "        x_copy = x\n",
    "        for name, module in self.conv_blocks[k].named_modules():\n",
    "          if name == 'conv2' or name == 'conv3':\n",
    "            x = x_copy + x # residual skip connection for the first two convs\n",
    "            x_copy = x.clone() # is it deep copy?\n",
    "          x = module(x)\n",
    "    x = self.final_convs(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = F.softmax(self.l1(x), -1)\n",
    "        \n",
    "    return x    \n",
    "        \n",
    "class SpatialAttention(nn.Module):\n",
    "  def __init__(self,in_channels, out_channels, K, x, y):\n",
    "    super().__init__()\n",
    "    self.outchans = out_channels\n",
    "    self.inchans = in_channels\n",
    "    self.K = K\n",
    "    self.x = x.to(device=device)\n",
    "    self.y = y.to(device=device)\n",
    "    self.x_drop = random.uniform(0, 1)\n",
    "    self.y_drop = random.uniform(0, 1)\n",
    "    self.compute_cos_sin()           \n",
    "    # trainable parameter:\n",
    "    self.z = Parameter(torch.randn(self.outchans, K*K, dtype = torch.cfloat,device=device)/(32*32)) # each output channel has its own KxK z matrix\n",
    "    self.z.requires_grad = True\n",
    "            \n",
    "  def compute_cos_sin(self):\n",
    "    kk = torch.arange(1, self.K+1, device=device)\n",
    "    ll = torch.arange(1, self.K+1, device=device)\n",
    "    cos_fun = lambda k, l, x, y: torch.cos(2*torch.pi*(k*x + l*y))\n",
    "    sin_fun = lambda k, l, x, y: torch.sin(2*torch.pi*(k*x + l*y))\n",
    "    self.cos_matrix = torch.stack([cos_fun(kk[None,:], ll[:,None], x, y) for x, y in zip(self.x, self.y)]).reshape(self.inchans,-1).float()\n",
    "    self.sin_matrix = torch.stack([sin_fun(kk[None,:], ll[:,None], x, y) for x, y in zip(self.x, self.y)]).reshape(self.inchans,-1).float()\n",
    "\n",
    "  def forward(self, X):            \n",
    "    a = torch.matmul(self.z.real, self.cos_matrix.T) + torch.matmul(self.z.imag, self.sin_matrix.T)\n",
    "    # Question: divide this with square root of KxK? to stablize gradient as with self-attention?\n",
    "    for i in range(self.inchans):\n",
    "      distance = (self.x_drop - self.x[i])**2 + (self.y_drop - self.y[i])**2\n",
    "      if distance < 0.1:\n",
    "        a[:, i] = 0\n",
    "        \n",
    "    a = F.softmax(a, dim=1) # softmax over all input chan location for each output chan\n",
    "                                            # outchans x  inchans\n",
    "                \n",
    "            # X: N x 273 x 360            \n",
    "    X = torch.matmul(a, X) # N x outchans x 360 (time)\n",
    "                                   # matmul dim expansion logic: https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "    return X\n",
    "\n",
    "\n",
    "class EEGDataset(Dataset):\n",
    "    '''\n",
    "    Custom Dataset object for PyTorch to load the dataset\n",
    "    '''\n",
    "    def __init__(self, x, y, train, val):\n",
    "        super(EEGDataset).__init__()\n",
    "        assert x.shape[0] == y.size\n",
    "        self.x = x\n",
    "        self.y = [y[i][0] for i in range(y.size)]\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "\n",
    "    def __getitem__(self,key):\n",
    "        return (self.x[key], self.y[key])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "\n",
    "def load_data(path, role, winLength, numChan, srate, feature, one_channel=False, version=\"\"):\n",
    "    \"\"\"\n",
    "    Load dataset\n",
    "    :param  \n",
    "        path: Filepath to the dataset\n",
    "        role: Role of the dataset. Can be \"train\", \"val\", or \"test\"\n",
    "        winLength: Length of time window. Can be 2 or 15\n",
    "        numChan: Number of channels. Can be 24 or 128\n",
    "        srate: Sampling rate. Supporting 126Hz\n",
    "        feature: Input feature. Can be \"raw\", \"spectral\", or \"topo\"\n",
    "        one_channel: Where input has 1 or 3 channel in depth dimension. Matters when load topo data as number of input channels \n",
    "                are different from original's\n",
    "        version: Any additional information of the datafile. Will be appended to the file name at the end\n",
    "    \"\"\"\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor()\n",
    "    ])\n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_x_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    x = f[f'X_{role}']\n",
    "    if feature == 'raw':\n",
    "        x = np.transpose(x,(0,2,1))\n",
    "        x = np.reshape(x,(-1,1,numChan,winLength*srate))\n",
    "    elif feature == 'topo':\n",
    "        if one_channel:\n",
    "            samples = []\n",
    "            for i in range(x.shape[0]):\n",
    "                image = x[i]\n",
    "                b, g, r = image[0,:, :], image[1,:, :], image[2,:, :]\n",
    "                concat = np.concatenate((b,g,r), axis=1)\n",
    "                samples.append(concat)\n",
    "            x = np.stack(samples)\n",
    "            x = np.reshape(x,(-1,1,x.shape[1],x.shape[2]))\n",
    "    \n",
    "    if version:\n",
    "        f = h5py.File(path + f\"child_mind_y_{role}_{winLength}s_{numChan}chan_{feature}_{version}.mat\", 'r')\n",
    "    else:\n",
    "        f = h5py.File(path + f\"child_mind_y_{role}_{winLength}s_{numChan}chan_{feature}.mat\", 'r')\n",
    "    y = f[f'Y_{role}']\n",
    "   \n",
    "    return EEGDataset(x, y, role=='train', role=='val')\n",
    "\n",
    "def create_original_model(feature):\n",
    "    if feature == 'raw':\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(1,100,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(100,100,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(100,300,(2,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(300,300,(1,7)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1,2), stride=1),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(300,100,(1,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(100,100,(1,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1900,6144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6144,2),\n",
    "        )\n",
    "    elif feature == 'topo':\n",
    "        model = nn.Sequential()\n",
    "        model.add_module('convolution', nn.Sequential(\n",
    "            nn.Conv2d(1,100,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(100,100,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(100,300,(2,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(300,300,(1,7),padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((1,2), stride=1),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(300,100,(1,3),padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(100,100,(1,3),padding=1),\n",
    "            nn.ReLU(),\n",
    "        ))\n",
    "        model.add_module('dense', nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1400,6144),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6144,2)\n",
    "        ))\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return Net(F_out = 120, inchans = 24, outchans = 24, K = 32)\n",
    "#     return create_original_model('raw')\n",
    "\n",
    "\n",
    "def check_accuracy(loader, model, device, dtype):\n",
    "    '''\n",
    "    Check accuracy of the model \n",
    "    param:\n",
    "        loader: An EEGDataset object\n",
    "        model: A PyTorch Module to test\n",
    "        device: cpu or cuda\n",
    "        dtype: value type\n",
    "        logger: Logger object for logging purpose\n",
    "    '''\n",
    "    \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return acc\n",
    "\n",
    "def train(model, loader_train, loader_val, optimizer, epochs):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    - logger: Logger object for logging purpose\n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    dtype = torch.float32\n",
    "    model.train()  # put model to training mode\n",
    "    loss_t = []\n",
    "    for e in range(epochs):\n",
    "        print(f'epoch {e}')\n",
    "        loss_train = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            loss_train = loss_train + loss\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "        total_sum = 0\n",
    "        with torch.no_grad():\n",
    "            for params in model.parameters():\n",
    "                total_sum += torch.sum(params)\n",
    "        print('model weights', total_sum)\n",
    "        print('train loss', loss_train)\n",
    "        loss_t.append(loss_train)\n",
    "\n",
    "        train_acc = check_accuracy(loader_train, model, device, dtype)\n",
    "        print('Train Accuracy at Epoch ' + str(e) + ': ' + str(train_acc)) \n",
    "        val_acc = check_accuracy(loader_val, model, device, dtype)\n",
    "        print('Val Accuracy at Epoch ' + str(e) + ': ' + str(val_acc))\n",
    "\n",
    "    print(loss_t)\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_experiment(seed, loader_train, loader_val, num_epoch):\n",
    "    model = create_model()\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    dtype = torch.float32\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # toggle between learning rate and batch size values \n",
    "    \n",
    "    optimizer = torch.optim.Adamax(model.parameters(), lr=0.002, weight_decay=0.001)\n",
    "    model = train(model, loader_train, loader_val, optimizer, epochs=num_epoch)\n",
    "\n",
    "\n",
    "    path = '/expanse/projects/nsg/external_users/public/arno/child_mind_abdu/'\n",
    "    winLength = 2\n",
    "    numChan = 24\n",
    "    srate = 128\n",
    "    feature = 'raw'\n",
    "    one_channel = False\n",
    "    \n",
    "    # Testing\n",
    "    test_data_balanced = load_data(path, 'test', winLength, numChan, srate, feature, False, 'v2')\n",
    "    sample_acc1, subject_acc1 = test_model(model, test_data_balanced, path + 'test_subjIDs.csv', device, dtype)\n",
    "    \n",
    "    print(sample_acc1)\n",
    "    print(subject_acc1)\n",
    "\n",
    "    test_data_all_male = load_data(path, 'test', winLength, numChan, srate, feature,False, 'v3')\n",
    "    sample_acc2, subject_acc2 = test_model(model, test_data_all_male, path + 'test_subjIDs_more_test.csv', device, dtype)\n",
    "\n",
    "    print(sample_acc2)\n",
    "    print(subject_acc2)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def test_model(model, test_data, subj_csv, device, dtype):\n",
    "    # one-segment test\n",
    "    loader_test = DataLoader(test_data, batch_size=70)\n",
    "    per_sample_acc = check_accuracy(loader_test, model, device, dtype)\n",
    "\n",
    "    # 40-segment test\n",
    "    with open(subj_csv, newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        subjIDs = [row[0] for row in spamreader]\n",
    "    unique_subjs,indices = np.unique(subjIDs,return_index=True)\n",
    "\n",
    "    iterable_test_data = list(iter(DataLoader(test_data, batch_size=1)))\n",
    "    num_correct = []\n",
    "    for subj,idx in zip(unique_subjs,indices):\n",
    "    #     print(f'Subj {subj} - gender {iterable_test_data[idx][1]}')\n",
    "        data = iterable_test_data[idx:idx+40]\n",
    "        #print(np.sum([y for _,y in data]))\n",
    "        assert 40 == np.sum([y for _,y in data]) or 0 == np.sum([y for _,y in data])\n",
    "        preds = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for x,y in data:\n",
    "                x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "                correct = y\n",
    "                scores = model(x)\n",
    "                _, pred = scores.max(1)\n",
    "                preds.append(pred)\n",
    "        final_pred = (torch.mean(torch.FloatTensor(preds)) > 0.5).sum()\n",
    "        num_correct.append((final_pred == correct).sum())\n",
    "    #print(len(num_correct))\n",
    "    acc = float(np.sum(num_correct)) / len(unique_subjs)\n",
    "    return per_sample_acc, acc\n",
    "\n",
    "\n",
    "def test_all_seeds(model_path, model_type, feature, test_data, subjIDs_file, epoch, num_seed, device, dtype, logger):\n",
    "    sample_acc = []\n",
    "    subject_acc = []\n",
    "    for s in range(num_seed):\n",
    "        model = create_model(model_type, feature)\n",
    "        model.load_state_dict(torch.load(f'{model_path}-seed{s}-epoch{epoch}'))\n",
    "        model.to(device=device)\n",
    "        sam_acc, sub_acc = test_model(model, test_data,subjIDs_file, device, dtype, logger)\n",
    "        sample_acc.append(sam_acc)\n",
    "        subject_acc.append(sub_acc)\n",
    "        \n",
    "    sample_acc = np.multiply(sample_acc,100)\n",
    "    subject_acc = np.multiply(subject_acc,100)\n",
    "    return sample_acc, subject_acc\n",
    "\n",
    "def get_stats(arr):\n",
    "    return np.min(arr), np.max(arr), np.mean(arr), np.std(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d18450",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = '/expanse/projects/nsg/external_users/public/arno/child_mind_abdu/'\n",
    "path = '/home/dtyoung/DL-EEG/data/'\n",
    "winLength = 2\n",
    "numChan = 24\n",
    "srate = 128\n",
    "feature = 'raw'\n",
    "one_channel = False\n",
    "\n",
    "role = 'train'\n",
    "train_data = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n",
    "print(f'X_train shape: {len(train_data)}, {train_data[0][0].shape}')\n",
    "print(f'Y_train shape: {len(train_data)}, {train_data[0][1].shape}')\n",
    "\n",
    "role = 'val'\n",
    "val_data = load_data(path, role, winLength, numChan, srate, feature, one_channel)\n",
    "print(f'X_val shape: {len(val_data)}, {val_data[0][0].shape}')\n",
    "print(f'Y_val shape: {len(val_data)}, {val_data[0][1].shape}')\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 70 \n",
    "loader_train = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "loader_val = DataLoader(val_data, batch_size=batch_size)\n",
    "\n",
    "seed = 1 # 0 worked\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "for s in range(1):\n",
    "    model = run_experiment(s, loader_train, loader_val, 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddfe0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
