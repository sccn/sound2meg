# -*- coding: utf-8 -*-
"""Spatial_Attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention.ipynb
"""

import torch
import os
from torch.autograd import Variable
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
import torch.optim as optim
from scipy.io import loadmat
#import h5py
from dataset_loading import Training_Sound2MEGDataset, Validation_Sound2MEGDataset, Test_Sound2MEGDataset
from torch.utils.data import Dataset, DataLoader, random_split
import gc
import numpy as np
from collections import OrderedDict
import random
from functools import partial
import logging
import math
import typing as tp

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
#device = torch.device('cpu')

class FourierEmb(nn.Module):
    """
    Fourier positional embedding.
    Unlike trad. embedding this is not using exponential periods
    for cosines and sinuses, but typical `2 pi k` which can represent
    any function over [0, 1]. As this function would be necessarily periodic,
    we take a bit of margin and do over [-0.2, 1.2].
    """
    def __init__(self, dimension: int = 288, margin: float = 0.2):
        super().__init__()
        n_freqs = (dimension // 2)**0.5
        assert int(n_freqs ** 2 * 2) == dimension
        self.dimension = dimension
        self.margin = margin

    def forward(self, positions):
        *O, D = positions.shape
        assert D == 2
        *O, D = positions.shape
        n_freqs = (self.dimension // 2)**0.5
        freqs_y = torch.arange(n_freqs).to(positions)
        freqs_x = freqs_y[:, None]
        width = 1 + 2 * self.margin
        positions = positions + self.margin
        p_x = 2 * math.pi * freqs_x / width
        p_y = 2 * math.pi * freqs_y / width
        positions = positions[..., None, None, :]
        loc = (positions[..., 0] * p_x + positions[..., 1] * p_y).reshape(*O, -1)
        emb = torch.cat([
            torch.cos(loc),
            torch.sin(loc),
        ], dim=-1)
        return emb

class ChannelMerger(nn.Module):
    def __init__(self, positions, chout: int, pos_dim: int = 288,
                 dropout: float = 0, usage_penalty: float = 0.,
                 n_subjects: int = 200, per_subject: bool = False):
        super().__init__()
        assert pos_dim % 4 == 0
        #self.position_getter = PositionGetter()
        self.positions = positions
        self.per_subject = per_subject
        if self.per_subject:
            self.heads = nn.Parameter(torch.randn(n_subjects, chout, pos_dim, requires_grad=True))
        else:
            self.heads = nn.Parameter(torch.randn(chout, pos_dim, requires_grad=True))
        self.heads.data /= pos_dim ** 0.5
        self.dropout = dropout
        self.embedding = FourierEmb(pos_dim)
        self.usage_penalty = usage_penalty
        self._penalty = torch.tensor(0.)

    @property
    def training_penalty(self):
        return self._penalty.to(next(self.parameters()).device)

    def forward(self, meg, subject_indices):
        B, C, T = meg.shape
        meg = meg.clone()
        positions = self.positions.expand(B, -1, -1)
        embedding = self.embedding(positions)
        score_offset = torch.zeros(B, C, device=meg.device)
        #score_offset[self.position_getter.is_invalid(positions)] = float('-inf')

        if self.training and self.dropout:
            center_to_ban = torch.rand(2, device=meg.device)
            radius_to_ban = self.dropout
            banned = (positions - center_to_ban).norm(dim=-1) <= radius_to_ban
            score_offset[banned] = float('-inf')

        if self.per_subject:
            _, cout, pos_dim = self.heads.shape
            subject = subject_indices
            heads = self.heads.gather(0, subject.view(-1, 1, 1).expand(-1, cout, pos_dim))
        else:
            heads = self.heads[None].expand(B, -1, -1)

        scores = torch.einsum("bcd,bod->boc", embedding, heads)
        scores += score_offset[:, None]
        weights = torch.softmax(scores, dim=2)
        out = torch.einsum("bct,boc->bot", meg, weights)
        if self.training and self.usage_penalty > 0.:
            usage = weights.mean(dim=(0, 1)).sum()
            self._penalty = self.usage_penalty * usage
        return out

class ChannelDropout(nn.Module):
    def __init__(self, positions, dropout: float = 0.1, rescale: bool = True):
        """
        Args:
            dropout: dropout radius in normalized [0, 1] coordinates.
            rescale: at valid, rescale all channels.
        """
        super().__init__()
        self.dropout = dropout
        self.rescale = rescale
        #self.position_getter = PositionGetter()
        self.positions = positions

    def forward(self, meg, batch):
        if not self.dropout:
            return meg

        B, C, T = meg.shape
        meg = meg.clone()
        positions = self.positions.expand(B, -1, -1)
        valid = (~self.position_getter.is_invalid(positions)).float()
        meg = meg * valid[:, :, None]

        if self.training:
            center_to_ban = torch.rand(2, device=meg.device)
            kept = (positions - center_to_ban).norm(dim=-1) > self.dropout
            meg = meg * kept.float()[:, :, None]
            if self.rescale:
                proba_kept = torch.zeros(B, C, device=meg.device)
                n_tests = 100
                for _ in range(n_tests):
                    center_to_ban = torch.rand(2, device=meg.device)
                    kept = (positions - center_to_ban).norm(dim=-1) > self.dropout
                    proba_kept += kept.float() / n_tests
                meg = meg / (1e-8 + proba_kept[:, :, None])

        return meg

class SimpleConv(nn.Module):
    def __init__(self,
                 montage,
                 # Channels
                 in_channels: tp.Dict[str, int],
                 out_channels: int,
                 hidden: tp.Dict[str, int],
                 # Overall structure
                 depth: int = 4,
                 concatenate: bool = False,  # concatenate the inputs
                 linear_out: bool = False,
                 complex_out: bool = False,
                 # Conv layer
                 kernel_size: int = 5,
                 growth: float = 1.,
                 dilation_growth: int = 2,
                 dilation_period: tp.Optional[int] = None,
                 skip: bool = False,
                 post_skip: bool = False,
                 scale: tp.Optional[float] = None,
                 rewrite: bool = False,
                 groups: int = 1,
                 glu: int = 0,
                 glu_context: int = 0,
                 glu_glu: bool = True,
                 gelu: bool = False,
                 # Dual path RNN
                 dual_path: int = 0,
                 # Dropouts, BN, activations
                 conv_dropout: float = 0.0,
                 dropout_input: float = 0.0,
                 batch_norm: bool = False,
                 relu_leakiness: float = 0.0,
                 # Subject specific settings
                 n_subjects: int = 200,
                 subject_dim: int = 64,
                 subject_layers: bool = False,
                 subject_layers_dim: str = "input",  # or hidden
                 subject_layers_id: bool = False,
                 embedding_scale: float = 1.0,
                 # stft transform
                 n_fft: tp.Optional[int] = None,
                 fft_complex: bool = True,
                 # Attention multi-dataset support
                 merger: bool = False,
                 merger_pos_dim: int = 256,
                 merger_channels: int = 270,
                 merger_dropout: float = 0.2,
                 merger_penalty: float = 0.,
                 merger_per_subject: bool = False,
                 dropout: float = 0.,
                 dropout_rescale: bool = True,
                 initial_linear: int = 0,
                 initial_depth: int = 1,
                 initial_nonlin: bool = False,
                 subsample_meg_channels: int = 0,
                 ):
        super().__init__()
        if set(in_channels.keys()) != set(hidden.keys()):
            raise ValueError("Channels and hidden keys must match "
                             f"({set(in_channels.keys())} and {set(hidden.keys())})")
        self._concatenate = concatenate
        self.out_channels = out_channels

        if gelu:
            activation = nn.GELU
        elif relu_leakiness:
            activation = partial(nn.LeakyReLU, relu_leakiness)
        else:
            activation = nn.ReLU

        assert kernel_size % 2 == 1, "For padding to work, this must be verified"

        self.merger = None
        self.dropout = None
        self.subsampled_meg_channels: tp.Optional[list] = None
        if subsample_meg_channels:
            assert 'meg' in in_channels
            indexes = list(range(in_channels['meg']))
            rng = random.Random(1234)
            rng.shuffle(indexes)
            self.subsampled_meg_channels = indexes[:subsample_meg_channels]

        self.initial_linear = None
        if dropout > 0.:
            self.dropout = ChannelDropout(montage, dropout, dropout_rescale)
        if merger:
            self.merger = ChannelMerger(
                montage, merger_channels, pos_dim=288, dropout=merger_dropout,
                usage_penalty=merger_penalty, n_subjects=n_subjects, per_subject=merger_per_subject)
            in_channels["meg"] = merger_channels

        if initial_linear:
            init = [nn.Conv1d(in_channels["meg"], initial_linear, 1)]
            for _ in range(initial_depth - 1):
                init += [activation(), nn.Conv1d(initial_linear, initial_linear, 1)]
            if initial_nonlin:
                init += [activation()]
            self.initial_linear = nn.Sequential(*init)
            in_channels["meg"] = initial_linear

        self.subject_layers = None
        if subject_layers:
            assert "meg" in in_channels
            meg_dim = in_channels["meg"]
            dim = {"hidden": hidden["meg"], "input": meg_dim}[subject_layers_dim]
            self.subject_layers = SubjectLayers(meg_dim, dim, n_subjects, subject_layers_id)
            in_channels["meg"] = dim

        self.stft = None
        if n_fft is not None:
            assert "meg" in in_channels
            self.fft_complex = fft_complex
            self.n_fft = n_fft
            self.stft = ta.transforms.Spectrogram(
                n_fft=n_fft,
                hop_length=n_fft//2,
                normalized=True,
                power=None if fft_complex else 1,
                return_complex=True)
            in_channels["meg"] *= n_fft // 2 + 1
            if fft_complex:
                in_channels["meg"] *= 2

        self.subject_embedding = None
        if subject_dim:
            self.subject_embedding = ScaledEmbedding(n_subjects, subject_dim, embedding_scale)
            in_channels["meg"] += subject_dim

        # concatenate inputs if need be
        if concatenate:
            in_channels = {"concat": sum(in_channels.values())}
            hidden = {"concat": sum(hidden.values())}

        # compute the sequences of channel sizes
        sizes = {}
        for name in in_channels:
            sizes[name] = [in_channels[name]]
            sizes[name] += [int(round(hidden[name] * growth ** k)) for k in range(depth)]

        params: tp.Dict[str, tp.Any]
        params = dict(kernel=kernel_size, stride=1,
                      leakiness=relu_leakiness, dropout=conv_dropout, dropout_input=dropout_input,
                      batch_norm=batch_norm, dilation_growth=dilation_growth, groups=groups,
                      dilation_period=dilation_period, skip=skip, post_skip=post_skip, scale=scale,
                      rewrite=rewrite, glu=glu, glu_context=glu_context, glu_glu=glu_glu,
                      activation=activation)

        final_channels = sum([x[-1] for x in sizes.values()])
        self.dual_path = None
        if dual_path:
            self.dual_path = DualPathRNN(final_channels, dual_path)
        self.final = None
        pad = 0
        kernel = 1
        stride = 1
        if n_fft is not None:
            pad = n_fft // 4
            kernel = n_fft
            stride = n_fft // 2

        if linear_out:
            assert not complex_out
            self.final = nn.ConvTranspose1d(final_channels, out_channels, kernel, stride, pad)
        elif complex_out:
            self.final = nn.Sequential(
                nn.Conv1d(final_channels, 2 * final_channels, 1),
                activation(),
                nn.ConvTranspose1d(2 * final_channels, out_channels, kernel, stride, pad))
        else:
            assert len(sizes) == 1, "if no linear_out, there must be a single branch."
            params['activation_on_last'] = False
            list(sizes.values())[0][-1] = out_channels

        self.encoders = nn.ModuleDict({name: ConvSequence(channels, **params)
                                       for name, channels in sizes.items()})

    def forward(self, inputs, subject_indices):
        subjects = subject_indices
        inputs = {"meg": inputs}
        length = next(iter(inputs.values())).shape[-1]  # length of any of the inputs

        if self.subsampled_meg_channels is not None:
            mask = torch.zeros_like(inputs["meg"][:1, :, :1])
            mask[:, self.subsampled_meg_channels] = 1.
            inputs["meg"] = inputs["meg"] * mask

        if self.dropout is not None:
            inputs["meg"] = self.dropout(inputs["meg"], batch)

        if self.merger is not None:
            inputs["meg"] = self.merger(inputs["meg"], subjects)

        if self.initial_linear is not None:
            inputs["meg"] = self.initial_linear(inputs["meg"])

        if self.subject_layers is not None:
            inputs["meg"] = self.subject_layers(inputs["meg"], subjects)

        if self.stft is not None:
            x = inputs["meg"]
            pad = self.n_fft // 4
            x = F.pad(pad_multiple(x, self.n_fft // 2), (pad, pad), mode='reflect')
            z = self.stft(inputs["meg"])
            B, C, Fr, T = z.shape
            if self.fft_complex:
                z = torch.view_as_real(z).permute(0, 1, 2, 4, 3)
            z = z.reshape(B, -1, T)
            inputs["meg"] = z

        if self.subject_embedding is not None:
            emb = self.subject_embedding(subjects)[:, :, None]
            inputs["meg"] = torch.cat([inputs["meg"], emb.expand(-1, -1, length)], dim=1)

        if self._concatenate:
            input_list = [x[1] for x in sorted(inputs.items())]
            inputs = {"concat": torch.cat(input_list, dim=1)}

        encoded = {}
        for name, x in inputs.items():
            encoded[name] = self.encoders[name](x)

        inputs = [x[1] for x in sorted(encoded.items())]
        x = torch.cat(inputs, dim=1)
        if self.dual_path is not None:
            x = self.dual_path(x)
        if self.final is not None:
            x = self.final(x)
        assert x.shape[-1] >= length
        return x[:, :, :length]
    
class SubjectLayers(nn.Module):
    """Per subject linear layer."""
    def __init__(self, in_channels: int, out_channels: int, n_subjects: int, init_id: bool = False):
        super().__init__()
        self.weights = nn.Parameter(torch.randn(n_subjects, in_channels, out_channels))
        if init_id:
            assert in_channels == out_channels
            self.weights.data[:] = torch.eye(in_channels)[None]
        self.weights.data *= 1 / in_channels**0.5

    def forward(self, x, subjects):
        _, C, D = self.weights.shape
        weights = self.weights.gather(0, subjects.view(-1, 1, 1).expand(-1, C, D))
        return torch.einsum("bct,bcd->bdt", x, weights)

    def __repr__(self):
        S, C, D = self.weights.shape
        return f"SubjectLayers({C}, {D}, {S})"

class ScaledEmbedding(nn.Module):
    """Scale up learning rate for the embedding, otherwise, it can move too slowly.
    """
    def __init__(self, num_embeddings: int, embedding_dim: int, scale: float = 10.):
        super().__init__()
        self.embedding = nn.Embedding(num_embeddings, embedding_dim)
        self.embedding.weight.data /= scale
        self.scale = scale

    @property
    def weight(self):
        return self.embedding.weight * self.scale

    def forward(self, x):
        return self.embedding(x) * self.scale

class ConvSequence(nn.Module):

    def __init__(self, channels: tp.Sequence[int], kernel: int = 4, dilation_growth: int = 1,
                 dilation_period: tp.Optional[int] = None, stride: int = 2,
                 dropout: float = 0.0, leakiness: float = 0.0, groups: int = 1,
                 decode: bool = False, batch_norm: bool = False, dropout_input: float = 0,
                 skip: bool = False, scale: tp.Optional[float] = None, rewrite: bool = False,
                 activation_on_last: bool = True, post_skip: bool = False, glu: int = 0,
                 glu_context: int = 0, glu_glu: bool = True, activation: tp.Any = None) -> None:
        super().__init__()
        dilation = 1
        channels = tuple(channels)
        self.skip = skip
        self.sequence = nn.ModuleList()
        self.glus = nn.ModuleList()
        if activation is None:
            activation = partial(nn.LeakyReLU, leakiness)
        Conv = nn.Conv1d if not decode else nn.ConvTranspose1d
        # build layers
        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):
            layers: tp.List[nn.Module] = []
            is_last = k == len(channels) - 2

            # Set dropout for the input of the conv sequence if defined
            if k == 0 and dropout_input:
                assert 0 < dropout_input < 1
                layers.append(nn.Dropout(dropout_input))

            # conv layer
            if dilation_growth > 1:
                assert kernel % 2 != 0, "Supports only odd kernel with dilation for now"
            if dilation_period and (k % dilation_period) == 0:
                dilation = 1
            pad = kernel // 2 * dilation
            layers.append(Conv(chin, chout, kernel, stride, pad,
                               dilation=dilation, groups=groups if k > 0 else 1))
            dilation *= dilation_growth
            # non-linearity
            if activation_on_last or not is_last:
                if batch_norm:
                    layers.append(nn.BatchNorm1d(num_features=chout))
                layers.append(activation())
                if dropout:
                    layers.append(nn.Dropout(dropout))
                if rewrite:
                    layers += [nn.Conv1d(chout, chout, 1), nn.LeakyReLU(leakiness)]
                    # layers += [nn.Conv1d(chout, 2 * chout, 1), nn.GLU(dim=1)]
            if chin == chout and skip:
                if scale is not None:
                    layers.append(LayerScale(chout, scale))
                if post_skip:
                    layers.append(Conv(chout, chout, 1, groups=chout, bias=False))

            self.sequence.append(nn.Sequential(*layers))
            if glu and (k + 1) % glu == 0:
                ch = 2 * chout if glu_glu else chout
                act = nn.GLU(dim=1) if glu_glu else activation()
                self.glus.append(
                    nn.Sequential(
                        nn.Conv1d(chout, ch, 1 + 2 * glu_context, padding=glu_context), act))
            else:
                self.glus.append(None)

    def forward(self, x: tp.Any) -> tp.Any:
        for module_idx, module in enumerate(self.sequence):
            old_x = x
            x = module(x)
            if self.skip and x.shape == old_x.shape:
                x = x + old_x
            glu = self.glus[module_idx]
            if glu is not None:
                x = glu(x)
        return x
       
def CLIP_loss(Z, Y):
    '''
    New loss using cross entropy implementation
    '''
    N = Y.size(dim = 0) # batch size
    Z_row = torch.reshape(Z, (N, -1)) # flatten to be N x F
    Y_row = F.normalize(torch.reshape(Y, (N, -1))) # flatten to be N x F
    inner_product = (torch.mm(Z_row, Y_row.T)).to(device)
    
    target = torch.arange(N, device=device)
    loss_brain = torch.nn.functional.cross_entropy(inner_product, target)
    loss_sound = torch.nn.functional.cross_entropy(inner_product.T, target)
    loss = ((loss_brain + loss_sound)/2).to(device)
    
    return loss

def find_indices(list_to_check, item_to_find):
    indices = []
    for idx, value in enumerate(list_to_check):
        if value == item_to_find:
            indices.append(idx)
    return indices
  

positions = loadmat('/expanse/projects/nsg/external_users/public/arno/electrode_positions.mat')
positions = torch.tensor(positions['positions'])
positions = positions.type(torch.FloatTensor)
positions = positions.to(device)

embedding_type = 'mel'
if embedding_type == 'mel':
  F_out = 120
elif embedding_type == 'Wav2Vec':
  F_out = 1024

training_data = Training_Sound2MEGDataset('/expanse/projects/nsg/external_users/public/', embedding_type)
validation_data = Validation_Sound2MEGDataset('/expanse/projects/nsg/external_users/public/', embedding_type)
test_data = Test_Sound2MEGDataset('/expanse/projects/nsg/external_users/public/', embedding_type)

Training_Data_Batches = DataLoader(training_data, batch_size = 128, shuffle = True)
Validation_Data_Batches = DataLoader(validation_data, batch_size = 128, shuffle = True)

#BrainModule = Net(F_out = F_out, inchans = 273, outchans = 270, K = 32, montage = positions, n_subjects = 125)
BrainModule = SimpleConv(positions, in_channels=dict(meg=273), hidden=dict(meg=125), out_channels=F_out, n_subjects=126, concatenate = False,depth= 4,linear_out= False,complex_out= False, kernel_size= 5, dilation_growth= 2,skip= False,post_skip=False,growth= 1.,rewrite= False,groups= 1,glu= 0,glu_context= 0,glu_glu= True,gelu= False,dual_path= 0,conv_dropout= 0.0,dropout_input= 0.0,batch_norm= False,relu_leakiness= 0.0,subject_dim= 0,subject_layers= True,embedding_scale= 1.0,subject_layers_dim= "input",subject_layers_id= False,fft_complex= True,merger= True,merger_pos_dim= 256,merger_channels= 270,merger_dropout= 0.2,merger_penalty= 0.,merger_per_subject= False,dropout= 0.,  dropout_rescale= True,initial_linear= 0,initial_depth= 1,initial_nonlin= False)
BrainModule.to(device)
#BrainModule.load_state_dict(torch.load('epochs162.pth'))
optimizer = optim.Adam(BrainModule.parameters(), lr = 0.0003)
loss_train = []
loss_val = []
training_accuracy = []
validation_accuracy = []

for i in range(51):
  loss_t = 0
  loss_v = 0
  train_acc = 0
  val_acc = 0
  BrainModule.train()
  for MEG, WAV, Sub, audio in Training_Data_Batches:
    Sub = Sub.tolist()
    Sub = torch.tensor(Sub, dtype=torch.int64).to(device)
    audio = list(audio)
    optimizer.zero_grad()
    Z = BrainModule(MEG.to(device).float(), Sub)
    loss = CLIP_loss(Z.float(), WAV.float().to(device))
    if i%10==0:
      N = Z.size(dim=0)
      Z_row = torch.reshape(Z.float(), (N, -1)).to(device)
      WAV_row = torch.reshape(WAV.float(), (N, -1)).to(device)
      inner_product = (torch.mm(Z_row, torch.transpose(WAV_row, 1, 0))/(N*N)).to(device)
      softm = torch.zeros(N, N).to(device)
      for j in range(N):
        softm[j] = nn.functional.softmax(inner_product[j, :], -1)
      Arguments = torch.argsort(softm, dim = 1, descending = True)
      k = 0
      for j in range(N):
        matches = find_indices(audio, audio[j])
        for m in matches:
          if m in Arguments[j, :10]:
            k = k+1
            break
      train_acc = train_acc + (k/N*100)
    loss.backward()
    loss_t = loss_t + loss.item()
    optimizer.step()
  loss_train.append(loss_t/(len(Training_Data_Batches)))
  if i%10 == 0:
    training_accuracy.append(train_acc/len(Training_Data_Batches))
  BrainModule.eval()
  for MEG_val, WAV_val, Sub_val, audio in Validation_Data_Batches:
    audio = list(audio)
    Sub_val = Sub_val.tolist()
    Sub_val = torch.tensor(Sub_val, dtype=torch.int64).to(device)
    with torch.no_grad():
      Z_val = BrainModule(MEG_val.to(device).float(), Sub_val)
      loss = CLIP_loss(Z_val.float(), WAV_val.float().to(device))
    if i%10==0:
      N = Z_val.size(dim=0)
      Z_row = torch.reshape(Z_val.float(), (N, -1)).to(device)
      WAV_row = torch.reshape(WAV_val.float(), (N, -1)).to(device)
      inner_product = (torch.mm(Z_row, torch.transpose(WAV_row, 1, 0))/(N*N)).to(device)
      softm = torch.zeros(N, N).to(device)
      for j in range(N):
        softm[j] = nn.functional.softmax(inner_product[j, :], -1)
      Arguments = torch.argsort(softm, dim = 1, descending = True)
      k = 0
      for j in range(N):
        matches = find_indices(audio, audio[j])
        for m in matches:
          if m in Arguments[j, :10]:
            k = k+1
            break
      val_acc = val_acc + (k/N*100)
    loss_v = loss_v + loss.item()
  loss_val.append(loss_v/len(Validation_Data_Batches))
  if i%10 == 0:
    validation_accuracy.append(val_acc/len(Validation_Data_Batches))
  gc.collect()
  torch.cuda.empty_cache()

print(loss_train)
print(loss_val)
print(training_accuracy)
print(validation_accuracy)

TestLoader = DataLoader(test_data, batch_size = 128)
Z_test = []
WAV_test = []
audios = []
for MEG_test_batch, WAV_test_batch, Sub_test_batch, audio in TestLoader:
  with torch.no_grad():
    Z_test_batch = BrainModule(MEG_test_batch.float().to(device), Sub_test_batch)
  Z_test.append(Z_test_batch)
  WAV_test.append(WAV_test_batch)
  audios.append(audio)

audio = []
for a in audios:
  for b in a:
    audio.append(b)

L = len(test_data)
Num_batches = len(TestLoader)
last_Zbatch = Z_test[Num_batches-1]
last_WAVbatch = WAV_test[Num_batches-1]
Z_test = torch.reshape(torch.stack(Z_test[0:Num_batches-1]).to(device), ((Num_batches-1)*128, F_out, 360))
Z_test = torch.cat((Z_test, last_Zbatch.to(device)), 0)
WAV_test = torch.reshape(torch.stack(WAV_test[0:Num_batches-1]).to(device), ((Num_batches-1)*128, F_out, 360))
WAV_test = torch.cat((WAV_test, last_WAVbatch.to(device)), 0)
Z_test_row = torch.reshape(Z_test.float(), (L, -1))
WAV_test_row = torch.reshape(WAV_test.float().to(device), (L, -1))
Product = (torch.mm(Z_test_row, WAV_test_row.T)/(L*L)).to(device)
softmax_product = torch.zeros(L, L).to(device)
for j in range(L):
  softmax_product[j] = nn.functional.softmax(Product[j, :], -1)

Arguments = torch.argsort(softmax_product, dim = 1, descending = True)
k = 0
for i in range(L):
  matches = find_indices(audio, audio[i])
  for m in matches:
    if m in Arguments[i, :10]:
      k = k+1
      break

print(k/L*100)

torch.save(BrainModule.state_dict(), 'epochs162.pth')
