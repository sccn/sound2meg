# -*- coding: utf-8 -*-
"""Spatial_Attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/sccn/sound2meg/blob/main/Spatial_Attention.ipynb
"""

import torch
import os
from torch.autograd import Variable
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
import torch.optim as optim
from scipy.io import loadmat
from dataset_loading import Sound2MEGDataset
from torch.utils.data import Dataset, DataLoader, random_split
import gc
import numpy as np
from collections import OrderedDict
from libs import BrainModule as Net

device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
          
def CLIP_loss(z, y):
    '''
    New loss using cross entropy implementation
    '''
    n = y.size(dim = 0) # batch size
    z_row = torch.reshape(z, (n, -1)) # flatten to be N x F
    y_row = torch.reshape(y, (n, -1)) # flatten to be N x F
    inner_product = (torch.mm(z_row, y_row.T)/(n*n)).to(device) # N x N. The normalization?

    target = torch.arange(n, device=device)
    loss_brain = torch.nn.functional.cross_entropy(inner_product, target)
    loss_sound = torch.nn.functional.cross_entropy(inner_product.T, target)
    loss = ((loss_brain + loss_sound)/2).to(device)
    
    return loss

positions = loadmat('/expanse/projects/nsg/external_users/public/arno/electrode_positions.mat')
positions = torch.tensor(positions['positions'])

embedding_type = 'mel'
if embedding_type == 'mel':
  f_out = 120
elif embedding_type == 'Wav2Vec':
  f_out = 1024
dataset = Sound2MEGDataset('/expanse/projects/nsg/external_users/public/', embedding_type)
training_data, validation_data, test_data = random_split(dataset, [11497, 3285, 1642], generator=torch.Generator().manual_seed(32))
training_Data_Batches = DataLoader(training_data, batch_size = 128, shuffle = True)
validation_Data_Batches = DataLoader(validation_data, batch_size = 128, shuffle = True)

BrainModule = Net(f_out = f_out, inchans = 273, outchans = 270, K = 32, montage = positions, n_subjects = 124)
BrainModule.to(device)
optimizer = optim.Adam(BrainModule.parameters(), lr = 0.00003)
loss_train = []
loss_val = []
training_accuracy = []
validation_accuracy = []

for i in range(100):
  loss_t = 0
  loss_v = 0
  train_acc = 0
  val_acc = 0
  BrainModule.train()
  for meg, wav, sub in training_Data_Batches:
    sub = sub.tolist()
    optimizer.zero_grad()
    z = BrainModule(meg.to(device).float(), sub)
    loss = CLIP_loss(z.float(), wav.float().to(device))
    if i%20==19:
      n = z.size(dim=0)
      z_row = torch.reshape(z.float(), (n, -1)).to(device)
      wav_row = torch.reshape(wav.float(), (n, -1)).to(device)
      inner_product = (torch.mm(z_row, torch.transpose(wav_row, 1, 0))/(N*N)).to(device)
      softm = torch.zeros(n, n).to(device)
      for j in range(n):
        softm[j] = nn.functional.softmax(inner_product[j, :], -1)
      arguments = torch.argsort(softm, dim = 1, descending = True)
      k = 0
      for j in range(n):
        if j in arguments[j, :10]:
          k = k+1
      train_acc = train_acc + (k/N*100)
    loss.backward()
    loss_t = loss_t + loss.item()
    optimizer.step()
  loss_train.append(loss_t/(len(training_Data_Batches)))
  if i%20 == 19:
    training_accuracy.append(train_acc/len(training_Data_Batches))
  BrainModule.eval()
  for meg_val, wav_val, sub_val in validation_Data_Batches:
    with torch.no_grad():
      z_val = BrainModule(meg_val.to(device).float(), sub_val)
      loss = CLIP_loss(z_val.float(), wav_val.float().to(device))
    if i%20==19:
      z = z_val.size(dim=0)
      z_row = torch.reshape(z_val.float(), (n, -1)).to(device)
      wav_row = torch.reshape(wav_val.float(), (n, -1)).to(device)
      inner_product = (torch.mm(z_row, torch.transpose(wav_row, 1, 0))/(n*n)).to(device)
      softm = torch.zeros(n, n).to(device)
      for j in range(n):
        softm[j] = nn.functional.softmax(inner_product[j, :], -1)
      arguments = torch.argsort(softm, dim = 1, descending = True)
      k = 0
      for j in range(n):
        if j in arguments[j, :10]:
          k = k+1
      val_acc = val_acc + (k/N*100)
    loss_v = loss_v + loss.item()
  loss_val.append(loss_v/len(validation_Data_Batches))
  if i%20 == 19:
    validation_accuracy.append(val_acc/len(validation_Data_Batches))
  gc.collect()
  torch.cuda.empty_cache()

print(loss_train)
print(loss_val)
print(training_accuracy)
print(validation_accuracy)

testLoader = DataLoader(test_data, batch_size = 128)
z_test = []
wav_test = []
for meg_test_batch, wav_test_batch, sub_test_batch in testLoader:
  with torch.no_grad():
    z_test_batch = BrainModule(meg_test_batch.float().to(device), sub_test_batch)
  z_test.append(z_test_batch)
  wav_test.append(wav_test_batch)

l = len(test_data)
num_batches = len(testLoader)
last_Zbatch = z_test[num_batches-1]
last_WAVbatch = wav_test[num_batches-1]
z_test = torch.reshape(torch.stack(z_test[0:Num_batches-1]).to(device), ((num_batches-1)*128, f_out, 360))
z_test = torch.cat((z_test, last_Zbatch.to(device)), 0)
wav_test = torch.reshape(torch.stack(wav_test[0:num_batches-1]).to(device), ((num_batches-1)*128, f_out, 360))
wav_test = torch.cat((wav_test, last_WAVbatch.to(device)), 0)
z_test_row = torch.reshape(z_test.float(), (L, -1))
wav_test_row = torch.reshape(wav_test.float().to(device), (L, -1))
product = (torch.mm(Z_test_row, WAV_test_row.T)/(L*L)).to(device)

softmax_product = torch.zeros(L, L).to(device)
for j in range(L):
  softmax_product[j] = nn.functional.softmax(product[j, :], -1)

arguments = torch.argsort(softmax_product, dim = 1, descending = True)
k = 0
for i in range(l):
  if i in arguments[i,:10]:
    k = k+1
print(k/l*100)

#torch.save(BrainModule.state_dict(), '/expanse/projects/nsg/external_users/public/arno/epochs200seed32.pth')
